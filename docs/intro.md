---
sidebar_position: 1
---

# LlamaEdge

LlamaEdge is the easiest & fastest way to run customized and fine-tuned LLMs locally or on the edge.

* Lightweight inference apps. LlamaEdge is in MBs instead of GBs
* Native and GPU accelerated performance
* Supports many GPU and hardware accelerators
* Supports many optimized inference libraries
* Wide selection of AI / LLM models

